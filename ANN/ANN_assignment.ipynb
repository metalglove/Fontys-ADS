{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600290873744",
   "display_name": "Python 3.7.9 64-bit ('fontys': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ANN - Exercise\n",
    "Construct, train and test an artificial neural network using a dataset of your own choice. Try different settings for two or more hyperparameters and investigate the effect on learning. Hand in a Jupyter notebook which contains your python code and in which you describe your approach and results. Also reflect on the knowledge and skills you acquired on artificial neural networks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manually setting the root directory to be Fontys\n",
    "import os\n",
    "# import sys\n",
    "# print(os.getcwd())\n",
    "root_path = os.path.split(os.getcwd())[0]\n",
    "assert root_path.endswith(\"/Fontys-ADS\"), \"The root path does not end with Fontys: \" + root_path \n",
    "sys.path.insert(0, root_path)"
   ]
  },
  {
   "source": [
    "## Preparing & Cleaning the data\n",
    "The dataset I have chosen is the Loan eligibility dataset from kaggle (https://www.kaggle.com/vikasukani/loan-eligible-dataset).<br/>\n",
    "I plan to predict whether someone is eligible for a loan."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      Loan_ID  Gender Married Dependents     Education Self_Employed  \\\n0    LP001002    Male      No          0      Graduate            No   \n1    LP001003    Male     Yes          1      Graduate            No   \n2    LP001005    Male     Yes          0      Graduate           Yes   \n3    LP001006    Male     Yes          0  Not Graduate            No   \n4    LP001008    Male      No          0      Graduate            No   \n..        ...     ...     ...        ...           ...           ...   \n609  LP002978  Female      No          0      Graduate            No   \n610  LP002979    Male     Yes         3+      Graduate            No   \n611  LP002983    Male     Yes          1      Graduate            No   \n612  LP002984    Male     Yes          2      Graduate            No   \n613  LP002990  Female      No          0      Graduate           Yes   \n\n     ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n0               5849                0.0         NaN             360.0   \n1               4583             1508.0       128.0             360.0   \n2               3000                0.0        66.0             360.0   \n3               2583             2358.0       120.0             360.0   \n4               6000                0.0       141.0             360.0   \n..               ...                ...         ...               ...   \n609             2900                0.0        71.0             360.0   \n610             4106                0.0        40.0             180.0   \n611             8072              240.0       253.0             360.0   \n612             7583                0.0       187.0             360.0   \n613             4583                0.0       133.0             360.0   \n\n     Credit_History Property_Area Loan_Status  \n0               1.0         Urban           Y  \n1               1.0         Rural           N  \n2               1.0         Urban           Y  \n3               1.0         Urban           Y  \n4               1.0         Urban           Y  \n..              ...           ...         ...  \n609             1.0         Rural           Y  \n610             1.0         Rural           Y  \n611             1.0         Urban           Y  \n612             1.0         Urban           Y  \n613             0.0     Semiurban           N  \n\n[614 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Loan_ID</th>\n      <th>Gender</th>\n      <th>Married</th>\n      <th>Dependents</th>\n      <th>Education</th>\n      <th>Self_Employed</th>\n      <th>ApplicantIncome</th>\n      <th>CoapplicantIncome</th>\n      <th>LoanAmount</th>\n      <th>Loan_Amount_Term</th>\n      <th>Credit_History</th>\n      <th>Property_Area</th>\n      <th>Loan_Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LP001002</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>5849</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LP001003</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>4583</td>\n      <td>1508.0</td>\n      <td>128.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LP001005</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>Yes</td>\n      <td>3000</td>\n      <td>0.0</td>\n      <td>66.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LP001006</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Not Graduate</td>\n      <td>No</td>\n      <td>2583</td>\n      <td>2358.0</td>\n      <td>120.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LP001008</td>\n      <td>Male</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>6000</td>\n      <td>0.0</td>\n      <td>141.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>609</th>\n      <td>LP002978</td>\n      <td>Female</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>2900</td>\n      <td>0.0</td>\n      <td>71.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>610</th>\n      <td>LP002979</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>3+</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>4106</td>\n      <td>0.0</td>\n      <td>40.0</td>\n      <td>180.0</td>\n      <td>1.0</td>\n      <td>Rural</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>LP002983</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>8072</td>\n      <td>240.0</td>\n      <td>253.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>612</th>\n      <td>LP002984</td>\n      <td>Male</td>\n      <td>Yes</td>\n      <td>2</td>\n      <td>Graduate</td>\n      <td>No</td>\n      <td>7583</td>\n      <td>0.0</td>\n      <td>187.0</td>\n      <td>360.0</td>\n      <td>1.0</td>\n      <td>Urban</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>LP002990</td>\n      <td>Female</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Graduate</td>\n      <td>Yes</td>\n      <td>4583</td>\n      <td>0.0</td>\n      <td>133.0</td>\n      <td>360.0</td>\n      <td>0.0</td>\n      <td>Semiurban</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n<p>614 rows Ã— 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# set the seed\n",
    "np.random.seed(56)\n",
    "\n",
    "# the loan_dataset_path.\n",
    "loan_dataset_path = \"dataset/loan-train.csv\"\n",
    "\n",
    "# reads the dataset from csv.\n",
    "df = pd.read_csv(loan_dataset_path)\n",
    "\n",
    "# displays the dataset.\n",
    "df"
   ]
  },
  {
   "source": [
    "By looking at the type of the columns, it can be seen that many columns still need to be cleaned up. </br>\n",
    "The Loan_ID column can be discarded since it only describes the loan as an unique identifier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Gender                object\nMarried               object\nDependents            object\nEducation             object\nSelf_Employed         object\nApplicantIncome        int64\nCoapplicantIncome    float64\nLoanAmount           float64\nLoan_Amount_Term     float64\nCredit_History       float64\nProperty_Area         object\nLoan_Status           object\ndtype: object"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# drops the Loan_ID column from the dataframe.\n",
    "df.drop(columns=['Loan_ID'], inplace=True)\n",
    "\n",
    "# prints the datatypes for each column.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Property_Area ['Urban' 'Rural' 'Semiurban']\nMarried ['No' 'Yes' nan]\nDependents ['0' '1' '2' '3+' nan]\nEducation ['Graduate' 'Not Graduate']\nGender ['Male' 'Female' nan]\nSelf_Employed ['No' 'Yes' nan]\nLoan_Status ['Y' 'N']\n"
    }
   ],
   "source": [
    "def one_hot_encode(df, column_name, drop_first=False):\n",
    "    # gets the unique values of the column.\n",
    "    uniques = df[column_name].unique()\n",
    "\n",
    "    # prints the unique values.\n",
    "    print(column_name, uniques)\n",
    "\n",
    "    # checks whether there is a NaN value in the uniques.\n",
    "    dummy_na = pd.isna(uniques).any()\n",
    "\n",
    "    # perform one-hot encoding. (drop_first for dummy encoding)\n",
    "    pa_dummies = pd.get_dummies(df[column_name], prefix=column_name, dummy_na=dummy_na, drop_first=drop_first)\n",
    "\n",
    "    # adds the one-hot encoded columns to the original dataframe.\n",
    "    df = pd.concat([df, pa_dummies], axis=1)\n",
    "\n",
    "    # drops the original column.\n",
    "    return df.drop([column_name], axis=1)\n",
    "\n",
    "\n",
    "# perform one-hot encoding on categorical columns.\n",
    "df = one_hot_encode(df, 'Property_Area')\n",
    "df = one_hot_encode(df, 'Married')\n",
    "df = one_hot_encode(df, 'Dependents')\n",
    "df = one_hot_encode(df, 'Education')\n",
    "df = one_hot_encode(df, 'Gender')\n",
    "df = one_hot_encode(df, 'Self_Employed')\n",
    "\n",
    "# performs dummy encoding by dropping the other column.\n",
    "# this is done to create a single predictable value.\n",
    "df = one_hot_encode(df, 'Loan_Status', drop_first=True)"
   ]
  },
  {
   "source": [
    "After creating all the categorical columns by one-hot encoding. </br> \n",
    "The last thing that needs to be done is to check whether the other values contain NaN values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ApplicantIncome             0\nCoapplicantIncome           0\nLoanAmount                 22\nLoan_Amount_Term           14\nCredit_History             50\nProperty_Area_Rural         0\nProperty_Area_Semiurban     0\nProperty_Area_Urban         0\nMarried_No                  0\nMarried_Yes                 0\nMarried_nan                 0\nDependents_0                0\nDependents_1                0\nDependents_2                0\nDependents_3+               0\nDependents_nan              0\nEducation_Graduate          0\nEducation_Not Graduate      0\nGender_Female               0\nGender_Male                 0\nGender_nan                  0\nSelf_Employed_No            0\nSelf_Employed_Yes           0\nSelf_Employed_nan           0\nLoan_Status_Y               0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "source": [
    "For now, let's just pad the missing data and see what the results are like. </br>\n",
    "If the predictions are really bad, this step could be tried with more attention to the datapoints that have missing data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ApplicantIncome            False\nCoapplicantIncome          False\nLoanAmount                 False\nLoan_Amount_Term           False\nCredit_History             False\nProperty_Area_Rural        False\nProperty_Area_Semiurban    False\nProperty_Area_Urban        False\nMarried_No                 False\nMarried_Yes                False\nMarried_nan                False\nDependents_0               False\nDependents_1               False\nDependents_2               False\nDependents_3+              False\nDependents_nan             False\nEducation_Graduate         False\nEducation_Not Graduate     False\nGender_Female              False\nGender_Male                False\nGender_nan                 False\nSelf_Employed_No           False\nSelf_Employed_Yes          False\nSelf_Employed_nan          False\nLoan_Status_Y              False\ndtype: bool\n"
    }
   ],
   "source": [
    "# interpolates the missing data by padding them with existing values.\n",
    "df['Loan_Amount_Term'].interpolate('pad', inplace=True)\n",
    "df['LoanAmount'].interpolate('pad', inplace=True)\n",
    "df['Credit_History'].interpolate('pad', inplace=True)\n",
    "\n",
    "# for some reason 1 record does not get padded, this way it will forcefully get padded.\n",
    "df['LoanAmount'].interpolate('bfill', inplace=True)\n",
    "\n",
    "# let's check if there are any NaN values left.S\n",
    "print(df.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensures that all values are computable by tensorflow. \n",
    "df['ApplicantIncome'] = df['ApplicantIncome'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gpu\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(56)\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets.base_dataset import DatasetBase\n",
    "\n",
    "# the loan dataset class.\n",
    "class LoanDataset(DatasetBase):\n",
    "    def __init__(self, df, batch_size, train_percentage, validation_percentage, test_percentage):\n",
    "        # sets the batch size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        features = tf.cast(df.loc[:, df.columns != 'Loan_Status_Y'].values, tf.float32)\n",
    "        labels = tf.cast(df.loc[:, 'Loan_Status_Y'].values, tf.bool)\n",
    "\n",
    "        # sets the data.\n",
    "        self.data = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "        # set the feature length.\n",
    "        self.feature_length = len(df.columns) - 1\n",
    "        \n",
    "        # shuffles the dataset\n",
    "        self.shuffle(256)\n",
    "\n",
    "        # splits the data into train, validation, and test datasets.\n",
    "        self.split_data_to_train_val_test(self.data, train_percentage, validation_percentage, test_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train: 123 validation: 41 test: 41\n"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "train_percentage = 0.6\n",
    "validation_percentage = 0.2\n",
    "test_percentage = 0.2\n",
    "loanDataset = LoanDataset(df, batch_size, train_percentage, validation_percentage, test_percentage)"
   ]
  },
  {
   "source": [
    "## Creating the ANN model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.base_model import ModelBase\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "\n",
    "class ANNModel(ModelBase):\n",
    "    def __init__(self, feature_length, gpu_initialized=False, training=False, limit=5000):\n",
    "        super().__init__(gpu_initialized, training, limit)\n",
    "\n",
    "        # sets the feature length for input.\n",
    "        self.feature_length = feature_length\n",
    "\n",
    "    def predict(self, X):\n",
    "        # create predictable array, since predicting only works on an array.\n",
    "        predictable_array = np.expand_dims(X, axis=0)\n",
    "\n",
    "        # perform prediction and take the first and only prediction out of the predictions array.\n",
    "        prediction = self.model.predict(X, verbose=1)[0]\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    def fit(self, training, callbacks, epochs, validation, validation_steps, steps_per_epoch):\n",
    "        self.model.fit(\n",
    "            training,\n",
    "            callbacks=callbacks,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation,\n",
    "            validation_steps=validation_steps,\n",
    "            steps_per_epoch=steps_per_epoch, verbose=2)\n",
    "\n",
    "    def compile(self, optimizer='adam', loss='mse', metrics=['mse'], loss_weights=[1.0], dense_units=2048, drop_out_rate=0.2, show_summary=False):\n",
    "        inputs = Input((self.feature_length,))\n",
    "\n",
    "        dense1 = Dense(dense_units, activation='relu', kernel_initializer='glorot_uniform')(inputs)\n",
    "        if self.training:\n",
    "            dense1 = Dropout(drop_out_rate)(dense1)\n",
    "        dense2 = Dense(dense_units, activation='relu', kernel_initializer='glorot_uniform')(dense1)\n",
    "        if self.training:\n",
    "            dense2 = Dropout(drop_out_rate)(dense2)\n",
    "        dense3 = Dense(dense_units, activation='relu', kernel_initializer='glorot_uniform')(dense2)\n",
    "        if self.training:\n",
    "            dense3 = Dropout(drop_out_rate)(dense3)\n",
    "        dense4 = Dense(dense_units, activation='relu', kernel_initializer='glorot_uniform')(dense3)\n",
    "        if self.training:\n",
    "            dense4 = Dropout(drop_out_rate)(dense4)\n",
    "        dense5 = Dense(dense_units, activation='relu', kernel_initializer='glorot_uniform')(dense4)\n",
    "        if self.training:\n",
    "            dense5 = Dropout(drop_out_rate)(dense5)\n",
    "        outputs = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')(dense5)\n",
    "\n",
    "        # construct the model by stitching the inputs and outputs\n",
    "        self.model = Model(inputs=inputs, outputs=outputs, name='ANNModel')\n",
    "\n",
    "        # compile the model\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics, loss_weights=loss_weights)\n",
    "\n",
    "        if show_summary:\n",
    "            self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANNModel(loanDataset.feature_length, training=True, gpu_initialized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"ANNModel\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 24)]              0         \n_________________________________________________________________\ndense (Dense)                (None, 2048)              51200     \n_________________________________________________________________\ndropout (Dropout)            (None, 2048)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2048)              4196352   \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 2048)              4196352   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 2048)              4196352   \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 2048)              4196352   \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 2049      \n=================================================================\nTotal params: 16,838,657\nTrainable params: 16,838,657\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import datetime\n",
    "\n",
    "epochs = 512\n",
    "INIT_LR = 1e-4\n",
    "opt = Adam(lr = INIT_LR, decay = INIT_LR / epochs)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mse', 'accuracy'], show_summary=True)\n",
    "\n",
    "# current time\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# create the checkpoint path\n",
    "checkpoint_path = \"checkpoints/ANNModel/\" + current_time + \".ckpt\"\n",
    "\n",
    "# create logging\n",
    "log_dir = \"logs/ANNModel/\" + current_time\n",
    "\n",
    "# create all callbacks\n",
    "callbacks = [\n",
    "  EarlyStopping(patience=50, monitor='val_loss'),\n",
    "  TensorBoard(log_dir=log_dir, profile_batch=0)#,\n",
    "  # ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/512\n123/123 - 1s - loss: 0.3171 - mse: 0.3171 - accuracy: 0.6829 - val_loss: 0.4065 - val_mse: 0.4065 - val_accuracy: 0.5935\nEpoch 2/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_mse: 0.3415 - val_accuracy: 0.6585\nEpoch 3/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_mse: 0.3171 - val_accuracy: 0.6829\nEpoch 4/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 5/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_mse: 0.3577 - val_accuracy: 0.6423\nEpoch 6/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 7/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 8/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_mse: 0.3415 - val_accuracy: 0.6585\nEpoch 9/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 10/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 11/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 12/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_mse: 0.3415 - val_accuracy: 0.6585\nEpoch 13/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_mse: 0.2602 - val_accuracy: 0.7398\nEpoch 14/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_mse: 0.2683 - val_accuracy: 0.7317\nEpoch 15/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 16/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 17/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3659 - val_mse: 0.3659 - val_accuracy: 0.6341\nEpoch 18/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3659 - val_mse: 0.3659 - val_accuracy: 0.6341\nEpoch 19/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_mse: 0.3415 - val_accuracy: 0.6585\nEpoch 20/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_mse: 0.2602 - val_accuracy: 0.7398\nEpoch 21/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 22/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 23/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_mse: 0.2683 - val_accuracy: 0.7317\nEpoch 24/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_mse: 0.2683 - val_accuracy: 0.7317\nEpoch 25/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 26/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2439 - val_mse: 0.2439 - val_accuracy: 0.7561\nEpoch 27/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 28/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_mse: 0.3577 - val_accuracy: 0.6423\nEpoch 29/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_mse: 0.2764 - val_accuracy: 0.7236\nEpoch 30/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_mse: 0.3577 - val_accuracy: 0.6423\nEpoch 31/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 32/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 33/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3740 - val_mse: 0.3740 - val_accuracy: 0.6260\nEpoch 34/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 35/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 36/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 37/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_mse: 0.2602 - val_accuracy: 0.7398\nEpoch 38/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 39/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 40/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_mse: 0.2927 - val_accuracy: 0.7073\nEpoch 41/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 42/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 43/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 44/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_mse: 0.2602 - val_accuracy: 0.7398\nEpoch 45/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 46/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 47/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 48/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 49/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3659 - val_mse: 0.3659 - val_accuracy: 0.6341\nEpoch 50/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 51/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_mse: 0.3415 - val_accuracy: 0.6585\nEpoch 52/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 53/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 54/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_mse: 0.3415 - val_accuracy: 0.6585\nEpoch 55/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_mse: 0.2927 - val_accuracy: 0.7073\nEpoch 56/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 57/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_mse: 0.2764 - val_accuracy: 0.7236\nEpoch 58/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 59/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 60/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2439 - val_mse: 0.2439 - val_accuracy: 0.7561\nEpoch 61/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2276 - val_mse: 0.2276 - val_accuracy: 0.7724\nEpoch 62/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_mse: 0.3496 - val_accuracy: 0.6504\nEpoch 63/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 64/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 65/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_mse: 0.2927 - val_accuracy: 0.7073\nEpoch 66/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 67/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 68/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3659 - val_mse: 0.3659 - val_accuracy: 0.6341\nEpoch 69/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3740 - val_mse: 0.3740 - val_accuracy: 0.6260\nEpoch 70/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 71/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_mse: 0.2927 - val_accuracy: 0.7073\nEpoch 72/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_mse: 0.2927 - val_accuracy: 0.7073\nEpoch 73/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_mse: 0.3171 - val_accuracy: 0.6829\nEpoch 74/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3984 - val_mse: 0.3984 - val_accuracy: 0.6016\nEpoch 75/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_mse: 0.3496 - val_accuracy: 0.6504\nEpoch 76/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 77/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 78/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 79/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_mse: 0.2602 - val_accuracy: 0.7398\nEpoch 80/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 81/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3984 - val_mse: 0.3984 - val_accuracy: 0.6016\nEpoch 82/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_mse: 0.2602 - val_accuracy: 0.7398\nEpoch 83/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_mse: 0.3333 - val_accuracy: 0.6667\nEpoch 84/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_mse: 0.3171 - val_accuracy: 0.6829\nEpoch 85/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 86/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_mse: 0.2683 - val_accuracy: 0.7317\nEpoch 87/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 88/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_mse: 0.3496 - val_accuracy: 0.6504\nEpoch 89/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 90/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_mse: 0.2927 - val_accuracy: 0.7073\nEpoch 91/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_mse: 0.3496 - val_accuracy: 0.6504\nEpoch 92/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 93/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2520 - val_mse: 0.2520 - val_accuracy: 0.7480\nEpoch 94/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 95/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 96/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.4065 - val_mse: 0.4065 - val_accuracy: 0.5935\nEpoch 97/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2358 - val_mse: 0.2358 - val_accuracy: 0.7642\nEpoch 98/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_mse: 0.2764 - val_accuracy: 0.7236\nEpoch 99/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_mse: 0.2764 - val_accuracy: 0.7236\nEpoch 100/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 101/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_mse: 0.3496 - val_accuracy: 0.6504\nEpoch 102/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_mse: 0.3008 - val_accuracy: 0.6992\nEpoch 103/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 104/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_mse: 0.3089 - val_accuracy: 0.6911\nEpoch 105/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_mse: 0.2927 - val_accuracy: 0.7073\nEpoch 106/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_mse: 0.3252 - val_accuracy: 0.6748\nEpoch 107/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_mse: 0.2764 - val_accuracy: 0.7236\nEpoch 108/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_mse: 0.3415 - val_accuracy: 0.6585\nEpoch 109/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 110/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_mse: 0.2846 - val_accuracy: 0.7154\nEpoch 111/512\n123/123 - 0s - loss: 0.3144 - mse: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_mse: 0.2764 - val_accuracy: 0.7236\n"
    }
   ],
   "source": [
    "# fit the model using the training data\n",
    "results = model.fit(\n",
    "  training=loanDataset.train_ds,\n",
    "  callbacks=callbacks,\n",
    "  epochs=epochs,\n",
    "  validation=loanDataset.val_ds,\n",
    "  validation_steps=loanDataset.val_size,\n",
    "  steps_per_epoch=loanDataset.train_size)\n",
    "\n",
    "weights_path = 'weights/ANNModel_trained_model_weights'\n",
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f0294718450> and <tensorflow.python.keras.layers.core.Dense object at 0x7f02ac13ad90>).\nWARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.core.Dense object at 0x7f02ac13ad90> and <tensorflow.python.keras.layers.core.Dense object at 0x7f02946e9c10>).\n\n# Evaluate on test data\n31/31 [==============================] - 0s 911us/step - loss: 0.3226 - mse: 0.3226\ntest loss, test acc: [0.32258063554763794, 0.32258063554763794]\n{'loss': 0.32258063554763794, 'mse': 0.32258063554763794}\n"
    }
   ],
   "source": [
    "# re initialize the model.\n",
    "model.training = False\n",
    "model.compile(optimizer=Adam(lr = 1e-4), loss='mse', metrics=['mse'], show_summary=False) \n",
    "model.load_weights(weights_path)\n",
    "\n",
    "print('\\n# Evaluate on test data')\n",
    "result = model.evaluate(loanDataset.actual_test_ds)\n",
    "print('test loss, test acc:', result)\n",
    "res = dict(zip(model.get_metric_names(), result))\n",
    "print(res)"
   ]
  },
  {
   "source": [
    "## Hypertuning\n",
    "Using automatic hypertuning with TensorFlow we can see what the best parameters would be."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32, 64, 128, 256]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1, 0.2, 0.3, 0.4]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))\n",
    "HP_LOSS = hp.HParam('loss', hp.Discrete(['mse', 'mae']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_LOSS],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = ANNModel(loanDataset.feature_length, gpu_initialized=True, training=True)\n",
    "    model.compile(\n",
    "        optimizer = hparams[HP_OPTIMIZER],\n",
    "        metrics=['accuracy'],\n",
    "        loss=hparams[HP_LOSS],\n",
    "        dense_units=hparams[HP_NUM_UNITS],\n",
    "        drop_out_rate=hparams[HP_DROPOUT]\n",
    "        )\n",
    "    results = model.fit(\n",
    "        training=loanDataset.train_ds,\n",
    "        callbacks=callbacks + [hp.KerasCallback(log_dir, hparams)],\n",
    "        epochs=epochs,\n",
    "        validation=loanDataset.val_ds,\n",
    "        validation_steps=loanDataset.val_size,\n",
    "        steps_per_epoch=loanDataset.train_size)\n",
    "\n",
    "    _, accuracy = model.evaluate(loanDataset.actual_test_ds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_results = []\n",
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "\n",
    "    # hparams.accuracy = accuracy\n",
    "    hp_results.append({'accuracy': accuracy, 'dense_units': hparams[HP_NUM_UNITS], \n",
    "      'dropout_rate': hparams[HP_DROPOUT], 'optimizer': hparams[HP_OPTIMIZER],\n",
    "      'loss': hparams[HP_LOSS] })\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "play_name=None, description=None): 'rmsprop', HParam(name='loss', domain=Discrete(['mae', 'mse']), display_name=None, description=None): 'mse'}\n--- Starting trial: run-117\n{'num_units': 256, 'dropout': 0.4, 'optimizer': 'rmsprop', 'loss': 'mse'}\nEpoch 1/512\n123/123 - 0s - loss: 0.3615 - accuracy: 0.6369 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 2/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_accuracy: 0.6423\nEpoch 3/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 4/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 5/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 6/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 7/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 8/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 9/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_accuracy: 0.6423\nEpoch 10/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 11/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 12/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.2602 - val_accuracy: 0.7398\nEpoch 13/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 14/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 15/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 16/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3821 - val_accuracy: 0.6179\nEpoch 17/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 18/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 19/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 20/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2439 - val_accuracy: 0.7561\nEpoch 21/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 22/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_accuracy: 0.6423\nEpoch 23/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3821 - val_accuracy: 0.6179\nEpoch 24/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 25/512\n123/123 - 0s - loss: 0.3117 - accuracy: 0.6883 - val_loss: 0.2114 - val_accuracy: 0.7886\nEpoch 26/512\n123/123 - 0s - loss: 0.3198 - accuracy: 0.6802 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 27/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 28/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_accuracy: 0.6585\nEpoch 29/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_accuracy: 0.7317\nEpoch 30/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 31/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 32/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_accuracy: 0.6423\nEpoch 33/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 34/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_accuracy: 0.7317\nEpoch 35/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_accuracy: 0.7317\nEpoch 36/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 37/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3821 - val_accuracy: 0.6179\nEpoch 38/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_accuracy: 0.7398\nEpoch 39/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3659 - val_accuracy: 0.6341\nEpoch 40/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_accuracy: 0.6585\nEpoch 41/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 42/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 43/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 44/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 45/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 46/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_accuracy: 0.6585\nEpoch 47/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_accuracy: 0.6423\nEpoch 48/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 49/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 50/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 51/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 52/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.4309 - val_accuracy: 0.5691\nEpoch 53/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 54/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 55/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 56/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 57/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 58/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 59/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 60/512\n123/123 - 0s - loss: 0.3117 - accuracy: 0.6883 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 61/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 62/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 63/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 64/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3740 - val_accuracy: 0.6260\nEpoch 65/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3659 - val_accuracy: 0.6341\nEpoch 66/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 67/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 68/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3821 - val_accuracy: 0.6179\nEpoch 69/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 70/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2520 - val_accuracy: 0.7480\nEpoch 71/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 72/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_accuracy: 0.7398\nEpoch 73/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 74/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 75/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\n31/31 [==============================] - 0s 758us/step - loss: 0.3871 - accuracy: 0.6129\n{HParam(name='num_units', domain=Discrete([16, 32, 64, 128, 256]), display_name=None, description=None): 256, HParam(name='dropout', domain=Discrete([0.1, 0.2, 0.3, 0.4]), display_name=None, description=None): 0.4, HParam(name='optimizer', domain=Discrete(['adam', 'rmsprop', 'sgd']), display_name=None, description=None): 'sgd', HParam(name='loss', domain=Discrete(['mae', 'mse']), display_name=None, description=None): 'mae'}\n--- Starting trial: run-118\n{'num_units': 256, 'dropout': 0.4, 'optimizer': 'sgd', 'loss': 'mae'}\nEpoch 1/512\n123/123 - 0s - loss: 0.3205 - accuracy: 0.6802 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 2/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 3/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 4/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_accuracy: 0.7317\nEpoch 5/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2439 - val_accuracy: 0.7561\nEpoch 6/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 7/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 8/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 9/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 10/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 11/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_accuracy: 0.7398\nEpoch 12/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 13/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_accuracy: 0.7317\nEpoch 14/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 15/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2602 - val_accuracy: 0.7398\nEpoch 16/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 17/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 18/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 19/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_accuracy: 0.7317\nEpoch 20/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 21/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 22/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_accuracy: 0.6585\nEpoch 23/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_accuracy: 0.6423\nEpoch 24/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3740 - val_accuracy: 0.6260\nEpoch 25/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3740 - val_accuracy: 0.6260\nEpoch 26/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 27/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 28/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 29/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3659 - val_accuracy: 0.6341\nEpoch 30/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 31/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 32/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_accuracy: 0.6585\nEpoch 33/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 34/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 35/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 36/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3821 - val_accuracy: 0.6179\nEpoch 37/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3902 - val_accuracy: 0.6098\nEpoch 38/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 39/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 40/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2520 - val_accuracy: 0.7480\nEpoch 41/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 42/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 43/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_accuracy: 0.6585\nEpoch 44/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 45/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 46/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 47/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 48/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 49/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.4390 - val_accuracy: 0.5610\nEpoch 50/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 51/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 52/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 53/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 54/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 55/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\n31/31 [==============================] - 0s 756us/step - loss: 0.3118 - accuracy: 0.6882\n{HParam(name='num_units', domain=Discrete([16, 32, 64, 128, 256]), display_name=None, description=None): 256, HParam(name='dropout', domain=Discrete([0.1, 0.2, 0.3, 0.4]), display_name=None, description=None): 0.4, HParam(name='optimizer', domain=Discrete(['adam', 'rmsprop', 'sgd']), display_name=None, description=None): 'sgd', HParam(name='loss', domain=Discrete(['mae', 'mse']), display_name=None, description=None): 'mse'}\n--- Starting trial: run-119\n{'num_units': 256, 'dropout': 0.4, 'optimizer': 'sgd', 'loss': 'mse'}\nEpoch 1/512\n123/123 - 0s - loss: 0.3168 - accuracy: 0.6829 - val_loss: 0.2602 - val_accuracy: 0.7398\nEpoch 2/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 3/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 4/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 5/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 6/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 7/512\n123/123 - 0s - loss: 0.3117 - accuracy: 0.6883 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 8/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 9/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 10/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2683 - val_accuracy: 0.7317\nEpoch 11/512\n123/123 - 0s - loss: 0.3117 - accuracy: 0.6883 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 12/512\n123/123 - 0s - loss: 0.3198 - accuracy: 0.6802 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 13/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 14/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 15/512\n123/123 - 0s - loss: 0.3117 - accuracy: 0.6883 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 16/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 17/512\n123/123 - 0s - loss: 0.3198 - accuracy: 0.6802 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 18/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 19/512\n123/123 - 0s - loss: 0.3089 - accuracy: 0.6911 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 20/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_accuracy: 0.6423\nEpoch 21/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 22/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3821 - val_accuracy: 0.6179\nEpoch 23/512\n123/123 - 0s - loss: 0.3117 - accuracy: 0.6883 - val_loss: 0.3821 - val_accuracy: 0.6179\nEpoch 24/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 25/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 26/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 27/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 28/512\n123/123 - 0s - loss: 0.3117 - accuracy: 0.6883 - val_loss: 0.2683 - val_accuracy: 0.7317\nEpoch 29/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 30/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 31/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 32/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3740 - val_accuracy: 0.6260\nEpoch 33/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 34/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 35/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2846 - val_accuracy: 0.7154\nEpoch 36/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3496 - val_accuracy: 0.6504\nEpoch 37/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3577 - val_accuracy: 0.6423\nEpoch 38/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 39/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3252 - val_accuracy: 0.6748\nEpoch 40/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 41/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3740 - val_accuracy: 0.6260\nEpoch 42/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\nEpoch 43/512\n123/123 - 0s - loss: 0.3117 - accuracy: 0.6883 - val_loss: 0.3089 - val_accuracy: 0.6911\nEpoch 44/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3333 - val_accuracy: 0.6667\nEpoch 45/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3171 - val_accuracy: 0.6829\nEpoch 46/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3415 - val_accuracy: 0.6585\nEpoch 47/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3984 - val_accuracy: 0.6016\nEpoch 48/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.2764 - val_accuracy: 0.7236\nEpoch 49/512\n123/123 - 0s - loss: 0.3171 - accuracy: 0.6829 - val_loss: 0.2927 - val_accuracy: 0.7073\nEpoch 50/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3659 - val_accuracy: 0.6341\nEpoch 51/512\n123/123 - 0s - loss: 0.3144 - accuracy: 0.6856 - val_loss: 0.3008 - val_accuracy: 0.6992\n31/31 [==============================] - 0s 815us/step - loss: 0.3226 - accuracy: 0.6774\n"
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for dropout_rate in HP_DROPOUT.domain.values:\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      for loss in HP_LOSS.domain.values:\n",
    "        hparams = {\n",
    "            HP_NUM_UNITS: num_units,\n",
    "            HP_DROPOUT: dropout_rate,\n",
    "            HP_OPTIMIZER: optimizer,\n",
    "            HP_LOSS: loss\n",
    "        }\n",
    "        print(hparams)\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        run('logs/hparam_tuning/' + run_name, hparams)\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-7340187c17e60b86\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-7340187c17e60b86\");\n          const url = new URL(\"/\", window.location);\n          url.port = 6006;\n          frame.src = url;\n        })();\n      </script>\n  "
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hp_results)\n",
    "\n",
    "def map_labels(row):\n",
    "    row['optimizer'] = HP_OPTIMIZER.domain.values.index(row['optimizer'])\n",
    "    row['loss'] = HP_LOSS.domain.values.index(row['loss'])\n",
    "    return row\n",
    "\n",
    "df = df.apply(map_labels, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "dimensions": [
          {
           "label": "Dense units",
           "range": [
            16,
            256
           ],
           "values": [
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            16,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            32,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            64,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            128,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256,
            256
           ]
          },
          {
           "label": "Dropout rate",
           "range": [
            0.1,
            0.4
           ],
           "values": [
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.1,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.3,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4,
            0.4
           ]
          },
          {
           "label": "Optimizer",
           "range": [
            0,
            3
           ],
           "ticktext": [
            "adam",
            "rmsprop",
            "sgd"
           ],
           "tickvals": [
            0,
            1,
            2
           ],
           "values": [
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2,
            0,
            0,
            1,
            1,
            2,
            2
           ]
          },
          {
           "label": "Loss",
           "range": [
            0,
            2
           ],
           "ticktext": [
            "mae",
            "mse"
           ],
           "tickvals": [
            0,
            1
           ],
           "values": [
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1,
            0,
            1
           ]
          },
          {
           "label": "Accuracy",
           "range": [
            0.2,
            0.8
           ],
           "values": [
            0.6666666865348816,
            0.6774193644523621,
            0.6666666865348816,
            0.698924720287323,
            0.7311828136444092,
            0.7311828136444092,
            0.698924720287323,
            0.6236559152603149,
            0.7096773982048035,
            0.7311828136444092,
            0.6774193644523621,
            0.6774193644523621,
            0.6236559152603149,
            0.7634408473968506,
            0.7096773982048035,
            0.602150559425354,
            0.7096773982048035,
            0.6559139490127563,
            0.6774193644523621,
            0.6774193644523621,
            0.7096773982048035,
            0.6559139490127563,
            0.6559139490127563,
            0.34408602118492126,
            0.6666666865348816,
            0.6451612710952759,
            0.698924720287323,
            0.6129032373428345,
            0.698924720287323,
            0.698924720287323,
            0.6774193644523621,
            0.698924720287323,
            0.6666666865348816,
            0.6881720423698425,
            0.7526881694793701,
            0.6559139490127563,
            0.6451612710952759,
            0.6666666865348816,
            0.6559139490127563,
            0.6774193644523621,
            0.6344085931777954,
            0.7311828136444092,
            0.7419354915618896,
            0.6774193644523621,
            0.7419354915618896,
            0.5913978219032288,
            0.6344085931777954,
            0.301075279712677,
            0.7096773982048035,
            0.6881720423698425,
            0.7526881694793701,
            0.7204301357269287,
            0.6774193644523621,
            0.301075279712677,
            0.7849462628364563,
            0.6666666865348816,
            0.6129032373428345,
            0.6666666865348816,
            0.32258063554763794,
            0.6451612710952759,
            0.6129032373428345,
            0.3655914068222046,
            0.6236559152603149,
            0.6451612710952759,
            0.7526881694793701,
            0.6774193644523621,
            0.6666666865348816,
            0.6559139490127563,
            0.602150559425354,
            0.7204301357269287,
            0.6774193644523621,
            0.7419354915618896,
            0.25806450843811035,
            0.698924720287323,
            0.6559139490127563,
            0.6129032373428345,
            0.29032257199287415,
            0.37634408473968506,
            0.7204301357269287,
            0.6451612710952759,
            0.7204301357269287,
            0.7311828136444092,
            0.7096773982048035,
            0.7096773982048035,
            0.6129032373428345,
            0.6774193644523621,
            0.6129032373428345,
            0.7311828136444092,
            0.3333333432674408,
            0.7204301357269287,
            0.6881720423698425,
            0.6451612710952759,
            0.6559139490127563,
            0.6881720423698425,
            0.6774193644523621,
            0.7096773982048035,
            0.6451612710952759,
            0.6559139490127563,
            0.7204301357269287,
            0.6344085931777954,
            0.6774193644523621,
            0.24731183052062988,
            0.6344085931777954,
            0.29032257199287415,
            0.7526881694793701,
            0.7849462628364563,
            0.31182795763015747,
            0.24731183052062988,
            0.29032257199287415,
            0.6451612710952759,
            0.6774193644523621,
            0.6666666865348816,
            0.25806450843811035,
            0.6881720423698425,
            0.6129032373428345,
            0.7849462628364563,
            0.6344085931777954,
            0.6129032373428345,
            0.6881720423698425,
            0.6774193644523621
           ]
          }
         ],
         "line": {
          "cmax": 0.8,
          "cmin": 0.2,
          "color": [
           0.6666666865348816,
           0.6774193644523621,
           0.6666666865348816,
           0.698924720287323,
           0.7311828136444092,
           0.7311828136444092,
           0.698924720287323,
           0.6236559152603149,
           0.7096773982048035,
           0.7311828136444092,
           0.6774193644523621,
           0.6774193644523621,
           0.6236559152603149,
           0.7634408473968506,
           0.7096773982048035,
           0.602150559425354,
           0.7096773982048035,
           0.6559139490127563,
           0.6774193644523621,
           0.6774193644523621,
           0.7096773982048035,
           0.6559139490127563,
           0.6559139490127563,
           0.34408602118492126,
           0.6666666865348816,
           0.6451612710952759,
           0.698924720287323,
           0.6129032373428345,
           0.698924720287323,
           0.698924720287323,
           0.6774193644523621,
           0.698924720287323,
           0.6666666865348816,
           0.6881720423698425,
           0.7526881694793701,
           0.6559139490127563,
           0.6451612710952759,
           0.6666666865348816,
           0.6559139490127563,
           0.6774193644523621,
           0.6344085931777954,
           0.7311828136444092,
           0.7419354915618896,
           0.6774193644523621,
           0.7419354915618896,
           0.5913978219032288,
           0.6344085931777954,
           0.301075279712677,
           0.7096773982048035,
           0.6881720423698425,
           0.7526881694793701,
           0.7204301357269287,
           0.6774193644523621,
           0.301075279712677,
           0.7849462628364563,
           0.6666666865348816,
           0.6129032373428345,
           0.6666666865348816,
           0.32258063554763794,
           0.6451612710952759,
           0.6129032373428345,
           0.3655914068222046,
           0.6236559152603149,
           0.6451612710952759,
           0.7526881694793701,
           0.6774193644523621,
           0.6666666865348816,
           0.6559139490127563,
           0.602150559425354,
           0.7204301357269287,
           0.6774193644523621,
           0.7419354915618896,
           0.25806450843811035,
           0.698924720287323,
           0.6559139490127563,
           0.6129032373428345,
           0.29032257199287415,
           0.37634408473968506,
           0.7204301357269287,
           0.6451612710952759,
           0.7204301357269287,
           0.7311828136444092,
           0.7096773982048035,
           0.7096773982048035,
           0.6129032373428345,
           0.6774193644523621,
           0.6129032373428345,
           0.7311828136444092,
           0.3333333432674408,
           0.7204301357269287,
           0.6881720423698425,
           0.6451612710952759,
           0.6559139490127563,
           0.6881720423698425,
           0.6774193644523621,
           0.7096773982048035,
           0.6451612710952759,
           0.6559139490127563,
           0.7204301357269287,
           0.6344085931777954,
           0.6774193644523621,
           0.24731183052062988,
           0.6344085931777954,
           0.29032257199287415,
           0.7526881694793701,
           0.7849462628364563,
           0.31182795763015747,
           0.24731183052062988,
           0.29032257199287415,
           0.6451612710952759,
           0.6774193644523621,
           0.6666666865348816,
           0.25806450843811035,
           0.6881720423698425,
           0.6129032373428345,
           0.7849462628364563,
           0.6344085931777954,
           0.6129032373428345,
           0.6881720423698425,
           0.6774193644523621
          ],
          "colorscale": [
           [
            0,
            "rgb(0, 147, 146)"
           ],
           [
            0.16666666666666666,
            "rgb(114, 170, 161)"
           ],
           [
            0.3333333333333333,
            "rgb(177, 199, 179)"
           ],
           [
            0.5,
            "rgb(241, 234, 200)"
           ],
           [
            0.6666666666666666,
            "rgb(229, 185, 173)"
           ],
           [
            0.8333333333333334,
            "rgb(217, 137, 148)"
           ],
           [
            1,
            "rgb(208, 88, 126)"
           ]
          ],
          "showscale": true
         },
         "type": "parcoords"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=\n",
    "    go.Parcoords(\n",
    "        line = dict(color = df['accuracy'],\n",
    "                   colorscale = 'Tealrose',\n",
    "                   showscale = True,\n",
    "                   cmin = 0.2,\n",
    "                   cmax = 0.8),\n",
    "        dimensions = list([\n",
    "            dict(range = [min(HP_NUM_UNITS.domain.values), max(HP_NUM_UNITS.domain.values)],\n",
    "                 label = \"Dense units\", values = df['dense_units']),\n",
    "            dict(range = [min(HP_DROPOUT.domain.values), max(HP_DROPOUT.domain.values)],\n",
    "                 label = 'Dropout rate', values = df['dropout_rate']),\n",
    "            dict(tickvals = [0, 1, 2],\n",
    "                 range = [0, len(HP_OPTIMIZER.domain.values)],\n",
    "                 ticktext = HP_OPTIMIZER.domain.values,\n",
    "                 label = 'Optimizer', values = df['optimizer']),\n",
    "            dict(tickvals = [0, 1],\n",
    "                 range = [0, len(HP_LOSS.domain.values)],\n",
    "                 ticktext = HP_LOSS.domain.values,\n",
    "                 label = 'Loss', values = df['loss']),\n",
    "            dict(range = [0.2, 0.8],\n",
    "                 label = 'Accuracy', values = df['accuracy'])])\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reset_labels(row):\n",
    "    row['optimizer'] = HP_OPTIMIZER.domain.values[int(row['optimizer'])]\n",
    "    row['loss'] = HP_LOSS.domain.values[int(row['loss'])]\n",
    "    return row\n",
    "\n",
    "df = df.apply(lambda row: reset_labels(row), axis='columns')"
   ]
  },
  {
   "source": [
    "## Results\n",
    "We can see that 3 models achieved the top score using different settings.\n",
    "It turned out that the settings I used for the first model in this notebook were not very favourable for the results.\n",
    "A lower amount of dense units will do the job just fine.\n",
    "Also, one thing to note is, that the data set I am using does not have that many records, so the accuracy will not be consistant.\n",
    "Since these models need to learn with a lot more data to be more accurate."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     accuracy  dense_units  dropout_rate optimizer loss\n54   0.784946         64.0           0.2      adam  mae\n105  0.784946        256.0           0.2   rmsprop  mse\n115  0.784946        256.0           0.4      adam  mse",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accuracy</th>\n      <th>dense_units</th>\n      <th>dropout_rate</th>\n      <th>optimizer</th>\n      <th>loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>54</th>\n      <td>0.784946</td>\n      <td>64.0</td>\n      <td>0.2</td>\n      <td>adam</td>\n      <td>mae</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>0.784946</td>\n      <td>256.0</td>\n      <td>0.2</td>\n      <td>rmsprop</td>\n      <td>mse</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>0.784946</td>\n      <td>256.0</td>\n      <td>0.4</td>\n      <td>adam</td>\n      <td>mse</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "ba[ba.accuracy == ba.accuracy.max()]"
   ]
  }
 ]
}